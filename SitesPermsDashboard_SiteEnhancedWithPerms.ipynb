{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%spark\r\n",
        "println(\"Application Id: \" + spark.sparkContext.applicationId )\r\n",
        "println(\"Application Name: \" + spark.sparkContext.appName)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 30,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:40:27.7006461Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:40:27.9520537Z",
              "execution_finish_time": "2023-11-17T10:40:29.0847344Z",
              "spark_jobs": null,
              "parent_msg_id": "a7f532c7-5e7b-4a2c-9dd7-f2f6c7f232ea"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 30, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Application Id: application_1700214569049_0001\nApplication Name: SitesPermsDashboard_SiteEnhancedWithPerms_sparkpoolag_1700214475\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up variables\r\n",
        "\r\n",
        "These initial variable values come from the notebook parameters"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "// var runId = \"93a19c30-e8b2-4af4-883b-8752fead65e4\"\r\n",
        "// val windowStartTime  = \"2023-10-31T00:00:00Z\"\r\n",
        "// val windowEndTime = \"2023-10-31T00:00:00Z\"\r\n",
        "// val storageAccountName = \"mgdcag\" \r\n",
        "// val storageContainerName = \"sites-permissions-dashbaord\"\r\n",
        "val retainForHistoricTrending: Boolean = true\r\n",
        "\r\n",
        "// If start and end date are the same we are performing a full pull\r\n",
        "val fullPull: Boolean = windowStartTime == windowEndTime\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 34,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:41:11.3544412Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:41:11.6192019Z",
              "execution_finish_time": "2023-11-17T10:41:13.5629024Z",
              "spark_jobs": null,
              "parent_msg_id": "0455dd9a-02d7-44b6-9793-3a19bb962073"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 34, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runId: String = 93a19c30-e8b2-4af4-883b-8752fead65e4\nwindowStartTime: String = 2023-10-31T00:00:00Z\nwindowEndTime: String = 2023-10-31T00:00:00Z\nstorageAccountName: String = mgdcag\nstorageContainerName: String = sites-permissions-dashbaord\nretainForHistoricTrending: Boolean = false\nfullPull: Boolean = true\n"
          ]
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import java.text.SimpleDateFormat\r\n",
        "import java.time.LocalDateTime\r\n",
        "import java.time.format.DateTimeFormatter\r\n",
        "import java.time.temporal.ChronoUnit\r\n",
        "import org.apache.spark.sql.types._\r\n",
        "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\r\n",
        "\r\n",
        "val standardDatePattern: String = \"yyyy-MM-dd'T'HH:mm:ss'Z'\"\r\n",
        "val windowStartDateTimeLocal: LocalDateTime =\r\n",
        "      LocalDateTime.parse(windowStartTime, DateTimeFormatter.ofPattern(standardDatePattern))\r\n",
        "val windowEndTimeLocal: LocalDateTime =\r\n",
        "      LocalDateTime.parse(windowEndTime, DateTimeFormatter.ofPattern(standardDatePattern))\r\n",
        "\r\n",
        "// set your storage account connection\r\n",
        "\r\n",
        "val timeDirFormatter = DateTimeFormatter.ofPattern(\"yyyy/MM/dd\")\r\n",
        "val yearMonthDayFormat = windowStartDateTimeLocal.format(timeDirFormatter).stripSuffix(\"/\")\r\n",
        "val yearMonthDayFormatEnd = windowEndTimeLocal.format(timeDirFormatter).stripSuffix(\"/\")\r\n",
        "\r\n",
        "val adls_path = f\"abfss://$storageContainerName@$storageAccountName.dfs.core.windows.net\"\r\n",
        "\r\n",
        "val spSites = adls_path + s\"/raw/sites/$yearMonthDayFormatEnd/$runId/\"\r\n",
        "val spPermissions = adls_path + s\"/raw/permissions/$yearMonthDayFormatEnd/$runId/\"\r\n",
        "\r\n",
        "val latestSPSitesEnhanced = adls_path + s\"/latest/sites/\"\r\n",
        "val latestSPPermissions = adls_path + s\"/latest/permissions/\"\r\n",
        "\r\n",
        "\r\n",
        "val sitesArchive = adls_path + s\"/archive/sites/$yearMonthDayFormatEnd/$runId/\"\r\n",
        "val permsArchive = adls_path + s\"/archive/permissions/$yearMonthDayFormatEnd/$runId/\"\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 56,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:54:25.9096252Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:54:26.0609912Z",
              "execution_finish_time": "2023-11-17T10:54:34.9644174Z",
              "spark_jobs": null,
              "parent_msg_id": "6a30c032-2467-4383-83f3-45168311df5f"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 56, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import java.text.SimpleDateFormat\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.time.temporal.ChronoUnit\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.{DataFrame, Row, SparkSession}\nstandardDatePattern: String = yyyy-MM-dd'T'HH:mm:ss'Z'\nwindowStartDateTimeLocal: java.time.LocalDateTime = 2023-10-31T00:00\nwindowEndTimeLocal: java.time.LocalDateTime = 2023-10-31T00:00\ntimeDirFormatter: java.time.format.DateTimeFormatter = Value(YearOfEra,4,19,EXCEEDS_PAD)'/'Value(MonthOfYear,2)'/'Value(DayOfMonth,2)\nyearMonthDayFormat: String = 2023/10/31\nyearMonthDayFormatEnd: String = 2023/10/31\nadls_path: String = abfss://sites-permissions-dashbaord@mgdcag.dfs.core.windows.net\nspSites: String = abfss://sites-permissions-dashbaord@mgdcag.dfs.core.windows.net/sites/2023/10/31/93a19c30-e8b2-4af4-883b-8752fead65e4/\nspPermissions: String = abfss://sites-permissions-dashbaord@mgdcag.dfs.core.windows.net/permissions/2023/10/31/93a19c30-e8b2-4af4-883b-8752fead65e4/\nlatestSPSitesEnhanced: String = abfss://sites-permissions-dashbaord@mgdcag.dfs.core.windows.net/latest/sites/\nlatestSPPermissions: String = abfss://sites-permissions-dashbaord@mgdcag.dfs.core.windows.net/latest/permissions/\nsitesArchive: String = abfss://sites-permissions-dashbaord@mgdcag.dfs.core.windows.net/archive/sites/2023/10/31/93a19c30-e8b2-4af4-883b-8752fead65e4/\npermsArchive: String = abfss://sites-permissions-dashbaord@mgdcag.dfs.core.windows.net/archive/permissions/2023/10/31/93a19c30-e8b2-4af4-883b-8752fead65e4/\n"
          ]
        }
      ],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the datasets into DFs (Data Frames)\r\n",
        "This are the files created by the MGDC copy tool"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val permissionsDF = \r\n",
        "    spark\r\n",
        "      .read\r\n",
        "      .format(\"json\")\r\n",
        "      .option(\"recursiveFileLookup\", \"false\")\r\n",
        "      .load(spPermissions)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 36,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:41:25.4667472Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:41:31.2393341Z",
              "execution_finish_time": "2023-11-17T10:41:33.1631655Z",
              "spark_jobs": null,
              "parent_msg_id": "2a797711-51af-41da-accb-34b098d2ec10"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 36, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "permissionsDF: org.apache.spark.sql.DataFrame = [FileExtension: string, ItemType: string ... 17 more fields]\n"
          ]
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val sitesDF =\r\n",
        "    spark\r\n",
        "      .read\r\n",
        "      .format(\"json\")\r\n",
        "      .option(\"recursiveFileLookup\", \"false\")\r\n",
        "      .load(spSites)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 37,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:41:25.529219Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:41:33.4192213Z",
              "execution_finish_time": "2023-11-17T10:41:35.4148312Z",
              "spark_jobs": null,
              "parent_msg_id": "44fc38f3-7715-4a17-a5dd-e39cec136ef3"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 37, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sitesDF: org.apache.spark.sql.DataFrame = [BlockAccessFromUnmanagedDevices: boolean, BlockDownloadOfAllFilesOnUnmanagedDevices: boolean ... 27 more fields]\n"
          ]
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val sitesCount = sitesDF.count()\r\n",
        "println(s\"The number of sites: $sitesCount\")\r\n",
        "\r\n",
        "val permissionsCount = permissionsDF.count()\r\n",
        "println(s\"The number of permissions objects: $permissionsCount\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 50,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:51:17.4046161Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:51:17.5605295Z",
              "execution_finish_time": "2023-11-17T10:51:21.687861Z",
              "spark_jobs": null,
              "parent_msg_id": "de11c0f9-4c5f-41b2-97d6-f738c603bc7b"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 50, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sites: 235\nThe number of permissions objects: 1153\nsitesCount: Long = 235\npermissionsCount: Long = 1153\n"
          ]
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enrich the Data\r\n",
        "\r\n",
        "Pretty sure this is called feature engineering \r\n",
        "\r\n",
        "## Add coloumns\r\n",
        "### Using UDFs (User-Defined Functions):\r\n",
        "You can define custom UDFs and use them to create new columns based on your specific logic. \r\n",
        "\r\n",
        "We will use this to add a boolean coloumn for OneDrive sites "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions._\r\n",
        "import org.apache.spark.sql.types._\r\n",
        "\r\n",
        "// returns true if site is OneDrive\r\n",
        "// Slighty different to the example above as I was getting scalla errors\r\n",
        "val isOneDrive = udf((siteUrl: String) => siteUrl.contains(\"-my.sharepoint.com\"))\r\n",
        "\r\n",
        "val sitesDFOD = sitesDF.withColumn(\"OneDriveSite\", isOneDrive($\"Url\"))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 52,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:53:07.9970606Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:53:08.1531139Z",
              "execution_finish_time": "2023-11-17T10:53:11.0047043Z",
              "spark_jobs": null,
              "parent_msg_id": "49381a3a-349d-4444-ba43-83b1e098c15c"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 52, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nisOneDrive: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$6708/25582520@1f8e042b,BooleanType,List(Some(class[value[0]: string])),Some(class[value[0]: boolean]),None,false,true)\nsitesDFOD: org.apache.spark.sql.DataFrame = [BlockAccessFromUnmanagedDevices: boolean, BlockDownloadOfAllFilesOnUnmanagedDevices: boolean ... 28 more fields]\n"
          ]
        }
      ],
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For joining permissions\r\n",
        "\r\n",
        "We need a way to join permissions. Unfortuantly there is no id so we need to make a composite id using the fileds are that are avalible\r\n",
        "\r\n",
        "the working theorey is that we can use `SiteId + ItemURL + RoleDefinition + LinkId`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions.udf\r\n",
        "import org.apache.spark.sql.DataFrame\r\n",
        "\r\n",
        "// Empty for permissions that are not links\r\n",
        "val defaultLinkValue = \"\"\r\n",
        "\r\n",
        "// Define the UDF for creating the composite key\r\n",
        "val createCompositeKey = udf((siteId: String, itemUrl: String, roleDefinition: String, linkId: String) =>\r\n",
        "  s\"$siteId-$itemUrl-$roleDefinition-${Option(linkId).getOrElse(defaultLinkValue)}\"\r\n",
        ")\r\n",
        "\r\n",
        "// Apply the UDF to create the composite key column\r\n",
        "val permissionsDFCK = permissionsDF.withColumn(\r\n",
        "  \"CompositeKey\",\r\n",
        "  createCompositeKey($\"SiteId\", $\"ItemURL\", $\"RoleDefinition\", $\"LinkId\")\r\n",
        ")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big Value Data Points\r\n",
        "\r\n",
        "This is where the real magic happens. With the data in the DF it's possible to work out previous version storage. What an insight, and we haven't even itterated every object.\r\n",
        "\r\n",
        "In the MGDC sites data set we can make the following assumption\r\n",
        "\r\n",
        "`PreviousVersionSize = TotalSize - TotalFileStreamSize - MetadataSize`\r\n",
        "\r\n",
        "With the addional data we now have we can make a far better assumption. We calcucate storage used in Drive by getting the size used by call the drives. We could probably even remove the metadata size\r\n",
        "\r\n",
        "`PreviousVersionSize = storageUsedInDrives - TotalFileStreamSize`\r\n",
        "\r\n",
        "This is just one example of what we can do with just a few extra toppings to add to this maverlous MGDC flavoured Pizza.\r\n",
        "\r\n",
        "We will use one of the UDFs from the start"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions._\r\n",
        "import org.apache.spark.sql.types._\r\n",
        "\r\n",
        "// returns true if site is OneDrive\r\n",
        "// Slighty different to the example above as I was getting scalla errors\r\n",
        "val previousVersionSize = udf((totalSize: BigInt, totalFileStreamSize: BigInt, metadataSize: BigInt) => \r\n",
        "    totalSize - totalFileStreamSize - metadataSize\r\n",
        ")\r\n",
        "// Assuming you have a DataFrame called \"df\"\r\n",
        "// TotalSize - TotalFileStreamSize - MetadataSize - storageUsedPreservationHold\r\n",
        "val sitesDFODPV = sitesDFOD\r\n",
        "    .withColumn(\"PreviousVersionSize\", previousVersionSize($\"StorageMetrics.TotalSize\", $\"StorageMetrics.TotalFileStreamSize\", $\"StorageMetrics.MetadataSize\"))\r\n",
        "\r\n",
        "val pvColoumns: DataFrame = sitesDFODPV.select(\"Id\", \"OneDriveSite\", \"PreviousVersionSize\")\r\n",
        "// using truncate = flase paramer to see full urls\r\n",
        "pvColoumns.show(20, truncate = false)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 53,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:53:08.0668205Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:53:11.1743044Z",
              "execution_finish_time": "2023-11-17T10:53:15.2380293Z",
              "spark_jobs": null,
              "parent_msg_id": "95fb108e-4183-4042-bb73-8f224f2f23a7"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 53, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+------------+-------------------+\n|Id                                  |OneDriveSite|PreviousVersionSize|\n+------------------------------------+------------+-------------------+\n|5b925130-3421-4b81-81cb-2b905b924ff3|false       |1599325            |\n|85b545cf-cde7-4ab8-8dec-abe4cb6fa377|false       |1522786            |\n|757b7c0a-7d92-4184-8c36-99fcf003bb51|false       |2124450            |\n|2be31aed-85d3-47f6-bc02-869575aea623|true        |1805365            |\n|a207386f-65da-47a7-ae97-0594fe1a4cae|false       |667519             |\n|1949f735-9790-410b-b6ad-a8f41475da3a|false       |5822548            |\n|62727fba-be2d-4ab6-903d-b79171ec9fa7|false       |1779858            |\n|93c3c5f1-c973-4a67-83b3-d37a741613d2|false       |1391780            |\n|c1c2d2f9-bf17-43d9-9a68-04dbd7fa8826|false       |1483342            |\n|8594a5ae-161c-4c5a-9dd1-b0d7b76c03ae|false       |2124607            |\n|8e1b2ba2-9df2-466b-bc8a-ca12bec60fad|false       |1477505            |\n|52922307-848f-4ee3-8286-5bd50c892aaa|false       |1484794            |\n|91d929df-7fbd-4934-9eef-01924afa543e|false       |1728295            |\n|c0f2ef4e-af75-4214-9441-95c3ed6b9581|true        |24538250           |\n|1efe2caa-11a6-4a58-8f98-2d83c8c7fefd|false       |2045524            |\n|439d2648-f990-4806-9099-35302c4958ee|false       |2462519            |\n|59ccc937-606a-4275-82aa-7b292bfdb8b1|false       |1724229            |\n|bab178f3-f732-49cb-b819-166308b155fc|false       |2731015            |\n|025ec5d6-b946-4545-ab28-9473cac8cd28|false       |1483127            |\n|5b12518e-6f2e-40cb-97ec-6cf6111271c3|false       |31778892           |\n+------------------------------------+------------+-------------------+\nonly showing top 20 rows\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\npreviousVersionSize: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$6709/1927331731@1dd54fcb,DecimalType(38,0),List(Some(class[value[0]: decimal(38,0)]), Some(class[value[0]: decimal(38,0)]), Some(class[value[0]: decimal(38,0)])),Some(class[value[0]: decimal(38,0)]),None,true,true)\nsitesDFODPV: org.apache.spark.sql.DataFrame = [BlockAccessFromUnmanagedDevices: boolean, BlockDownloadOfAllFilesOnUnmanagedDevices: boolean ... 29 more fields]\npvColoumns: org.apache.spark.sql.DataFrame = [Id: string, OneDriveSite: boolean ... 1 more field]\n"
          ]
        }
      ],
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Pull or Delta?\r\n",
        "\r\n",
        "Do we need to merge the new data with the exising latest.\r\n",
        "\r\n",
        "If we are perofrming a full pull then we do not. If this is a delta pull then we do\r\n",
        "\r\n",
        "We will pull in the previous latest dataset (which will be a complete picture as of the last scan)\r\n",
        "\r\n",
        "```scala\r\n",
        "// Specify the join condition based on the common key(s)\r\n",
        "val joinCondition = Seq(\"commonKeyColumn\")\r\n",
        "\r\n",
        "// Perform a left-join to keep all rows from existingDF and overwrite with values from deltaDF where commonKeyColumn matches\r\n",
        "val mergedDF = existingDF.join(deltaDF, joinCondition, \"left_outer\")\r\n",
        "  .select(existingDF.columns.map(colName => coalesce(deltaDF(colName), existingDF(colName)).alias(colName)): _*)\r\n",
        "\r\n",
        "// Now, mergedDF contains the updated values from deltaDF where available, and the original values where not updated.\r\n",
        "\r\n",
        "```\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perms"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Define a function that takes a DataFrame as a parameter and returns a DataFrame\r\n",
        "def createFullPermissionsDataFrame(): DataFrame = {\r\n",
        "\r\n",
        "    if (fullPull) {\r\n",
        "        println(\"Full pull, no need to merge deltas\")\r\n",
        "\r\n",
        "        return permissionsDFCK\r\n",
        "\r\n",
        "    } else {\r\n",
        "        println(\"Delta pull, we must merge\")\r\n",
        "\r\n",
        "        // so for sites the existing will be the latest\r\n",
        "        val existingPermsDF =\r\n",
        "            spark\r\n",
        "            .read\r\n",
        "            .format(\"json\")\r\n",
        "            .option(\"recursiveFileLookup\", \"false\")\r\n",
        "            .load(latestSPPermissions)\r\n",
        "\r\n",
        "        // the delta will be the DF from the current pull\r\n",
        "        val deltaPermsDF = permissionsDFCK\r\n",
        "\r\n",
        "        // JOin condition will be site Id\r\n",
        "        val permsJoinCondition = Seq(\"CompositeKey\")\r\n",
        "\r\n",
        "        // Perform a left-join to keep all rows from existingDF and overwrite with values from deltaDF where commonKeyColumn matches\r\n",
        "        val mergedPermsDF = existingPermsDF.join(deltaPermsDF, permsJoinCondition, \"left_outer\")\r\n",
        "            .select(existingPermsDF.columns.map(colName => coalesce(deltaPermsDF(colName), existingPermsDF(colName)).alias(colName)): _*)\r\n",
        "\r\n",
        "        // Set sites DF to the new merged\r\n",
        "        val fullPerms: DataFrame = mergedPermsDF\r\n",
        "\r\n",
        "\r\n",
        "        return fullPerms\r\n",
        "\r\n",
        "    }\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Define a function that takes a DataFrame as a parameter and returns a DataFrame\r\n",
        "def createFullSitesDataFrame(): DataFrame = {\r\n",
        "\r\n",
        "    if (fullPull) {\r\n",
        "        println(\"Full pull, no need to merge deltas\")\r\n",
        "\r\n",
        "        return sitesDFODPV\r\n",
        "\r\n",
        "    } else {\r\n",
        "        println(\"Delta pull, we must merge\")\r\n",
        "\r\n",
        "        // so for sites the existing will be the latest\r\n",
        "        val existingSitesDF =\r\n",
        "            spark\r\n",
        "            .read\r\n",
        "            .format(\"json\")\r\n",
        "            .option(\"recursiveFileLookup\", \"false\")\r\n",
        "            .load(latestSPSitesEnhanced)\r\n",
        "\r\n",
        "        // the delta will be the DF from the current pull\r\n",
        "        val deltaSitesDF = sitesDFODPV\r\n",
        "\r\n",
        "        // JOin condition will be site Id\r\n",
        "        val sitesJoinCondition = Seq(\"Id\")\r\n",
        "\r\n",
        "        // Perform a left-join to keep all rows from existingDF and overwrite with values from deltaDF where commonKeyColumn matches\r\n",
        "        val mergedSitesDF = existingSitesDF.join(deltaSitesDF, sitesJoinCondition, \"left_outer\")\r\n",
        "            .select(existingSitesDF.columns.map(colName => coalesce(deltaSitesDF(colName), existingSitesDF(colName)).alias(colName)): _*)\r\n",
        "\r\n",
        "        // Set sites DF to the new merged\r\n",
        "        val fullSites: DataFrame = mergedSitesDF\r\n",
        "\r\n",
        "\r\n",
        "        return fullSites\r\n",
        "\r\n",
        "        // For permissions - parametrise to make this better\r\n",
        "        // so for permissions the existing will be the latest\r\n",
        "        // val existingPermsDF =\r\n",
        "        //     spark\r\n",
        "        //     .read\r\n",
        "        //     .format(\"json\")\r\n",
        "        //     .option(\"recursiveFileLookup\", \"false\")\r\n",
        "        //     .load(latestSPPermissions)\r\n",
        "\r\n",
        "        // // the delta will be the DF from the current pull\r\n",
        "        // val deltaPermsDF = permissionsDF\r\n",
        "\r\n",
        "        // // JOin condition will be site Id\r\n",
        "        // val sitesJoinCondition = Seq(\"Id\")\r\n",
        "\r\n",
        "        // // Perform a left-join to keep all rows from existingDF and overwrite with values from deltaDF where commonKeyColumn matches\r\n",
        "        // val mergedSitesDF = existingSitesDF.join(deltaSitesDF, sitesJoinCondition, \"left_outer\")\r\n",
        "        //     .select(existingDF.columns.map(colName => coalesce(deltaSitesDF(colName), existingSitesDF(colName)).alias(colName)): _*)\r\n",
        "\r\n",
        "        // // Set sites DF to the new merged\r\n",
        "        // sitesDF = mergedSitesDF\r\n",
        "\r\n",
        "    }\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop deletes\r\n",
        "\r\n",
        "We need to drop any delete operations. Or they can be kept to see number of items deleted. Could be useful?"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val fullSitesDF: DataFrame = createFullSitesDataFrame()\r\n",
        "\r\n",
        "val fullSitesCount = fullSitesDF.count()\r\n",
        "println(s\"The number of sites: $fullSitesCount\")\r\n",
        "\r\n",
        "val fullPermsDF: DataFrame = createFullPermissionsDataFrame()\r\n",
        "\r\n",
        "val fullPermsCount = fullPermsDF.count()\r\n",
        "println(s\"The number of perms: $fullPermsCount\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write back to blob storage\r\n",
        "\r\n",
        "We need to write our new dataset back to the blobs - We will drop it in the latest folder. This will make the PowerBI end easier.\r\n",
        "\r\n",
        "We also need to write out blob to archive to retiain. As the latest folder it ovwer written on every new run"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//val latestSitesEnhanced = adls_path + s\"/sitesenhanced/latest/\"\r\n",
        "fullSitesDF\r\n",
        "    .repartition(1)\r\n",
        "    .write\r\n",
        "    .format(\"json\")\r\n",
        "    .mode(\"overwrite\")\r\n",
        "    .save(latestSPSitesEnhanced)\r\n",
        "\r\n",
        "\r\n",
        "fullPermsDF\r\n",
        "    .repartition(1)\r\n",
        "    .write\r\n",
        "    .format(\"json\")\r\n",
        "    .mode(\"overwrite\")\r\n",
        "    .save(latestSPPermissions)\r\n",
        "\r\n",
        "\r\n",
        "if (retainForHistoricTrending) {\r\n",
        "    fullSitesDF\r\n",
        "        .repartition(1)\r\n",
        "        .write\r\n",
        "        .format(\"json\")\r\n",
        "        .mode(\"overwrite\")\r\n",
        "        .save(sitesArchive)\r\n",
        "\r\n",
        "    fullPermsDF\r\n",
        "        .repartition(1)\r\n",
        "        .write\r\n",
        "        .format(\"json\")\r\n",
        "        .mode(\"overwrite\")\r\n",
        "        .save(permsArchive)\r\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "25",
              "statement_id": 57,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-17T10:55:42.4592559Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-17T10:55:42.6871928Z",
              "execution_finish_time": "2023-11-17T10:55:48.167162Z",
              "spark_jobs": null,
              "parent_msg_id": "38ec3313-333c-4a33-b8e7-52ee77dcae4d"
            },
            "text/plain": "StatementMeta(sparkpoolag, 25, 57, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}